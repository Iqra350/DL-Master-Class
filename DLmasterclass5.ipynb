{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLmasterclass5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7LUMeeJT80D"
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, DenseFeatures\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwxvoRIaVUtW"
      },
      "source": [
        "# Now read the CSV files\n",
        "colums = [\n",
        "    'fare_amount',\n",
        "    'pickup_datetime',\n",
        "    'pickup_longitude',\n",
        "    'pickup_latitude',\n",
        "    'dropoff_longitude',\n",
        "    'dropoff_latitude',\n",
        "    'passenger_count',\n",
        "    'key'\n",
        "]\n",
        "label_colums = 'fare_amount'\n",
        "unwanted_coums = ['pickup_datetime','key']\n",
        "defaults = [[0.0],['na'],[0.0],[0.0],[0.0],[0.0],[0.0],['na']]\n",
        "def feature_and_labels(row_data):\n",
        "  label = row_data.pop(label_colums)\n",
        "  features = row_data\n",
        "  for unwanted_coloms in unwanted_coums:\n",
        "    features.pop(unwanted_coloms)\n",
        "  return features, label\n",
        "def create_dataset(pattern,batch_size=1,mode='eval'):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(pattern,batch_size,colums,defaults)\n",
        "  dataset = dataset.map(feature_and_labels)\n",
        "  if mode == 'train':\n",
        "    dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKk0Z9l-YpYR"
      },
      "source": [
        "INPUT_COLS = [\n",
        "    'pickup_longitude',\n",
        "    'pickup_latitude',\n",
        "    'dropoff_longitude',\n",
        "    'dropoff_latitude',\n",
        "    'passenger_count',\n",
        "]\n",
        "features_coloms = {colname: tf.feature_column.numeric_column(colname)\n",
        "                    for colname in INPUT_COLS\n",
        "                   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuEDBVHaKT9"
      },
      "source": [
        "#Building a simple sequential model\n",
        "model = Sequential([\n",
        "                   DenseFeatures(feature_columns= features_coloms.values()),\n",
        "                   Dense(units = 32, activation='relu',name='h1'),\n",
        "                   Dense(units=8, activation='relu', name='h2'),\n",
        "                   Dense(units=1, activation='linear', name='ouput')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRhu611sbF7w"
      },
      "source": [
        "#create a custom evaluation matric\n",
        "def rmse(y_pred,y_true):\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(y_pred-y_true)))\n",
        "model.compile(optimizer='adam',loss='mse',metrics=[rmse,'mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHyGFnwDcgKw"
      },
      "source": [
        "train_batch_size = 1000\n",
        "train_example = 10000 * 5\n",
        "num_eval = 50\n",
        "num_eval_example = 10000\n",
        "train_data = create_dataset('/content/drive/MyDrive/Colab Notebooks/Data sets/05_Tensorflow_Keras_Sequential/taxi-train.csv',\n",
        "                            batch_size=train_batch_size,\n",
        "                            mode = 'train')\n",
        "eval_data = create_dataset('/content/drive/MyDrive/Colab Notebooks/Data sets/05_Tensorflow_Keras_Sequential/taxi-valid.csv',\n",
        "                           batch_size=1000,\n",
        "                           mode = 'eval')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFee4QdEeCrR",
        "outputId": "2aeeb7c6-ca3a-485c-ee6d-b6b57ef27c79"
      },
      "source": [
        "%time\n",
        "setps_per_epoch = train_example // (train_batch_size * num_eval)\n",
        "logdir = './taxi_trained'\n",
        "history = model.fit(x=train_data,\n",
        "                    steps_per_epoch = setps_per_epoch,\n",
        "                    epochs = num_eval,\n",
        "                    validation_data = eval_data,\n",
        "                    callbacks=[TensorBoard(log_dir=logdir)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
            "Wall time: 4.53 µs\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('pickup_longitude', <tf.Tensor 'ExpandDims_4:0' shape=(1000, 1) dtype=float32>), ('pickup_latitude', <tf.Tensor 'ExpandDims_3:0' shape=(1000, 1) dtype=float32>), ('dropoff_longitude', <tf.Tensor 'ExpandDims_1:0' shape=(1000, 1) dtype=float32>), ('dropoff_latitude', <tf.Tensor 'ExpandDims:0' shape=(1000, 1) dtype=float32>), ('passenger_count', <tf.Tensor 'ExpandDims_2:0' shape=(1000, 1) dtype=float32>)])\n",
            "Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('pickup_longitude', <tf.Tensor 'ExpandDims_4:0' shape=(1000, 1) dtype=float32>), ('pickup_latitude', <tf.Tensor 'ExpandDims_3:0' shape=(1000, 1) dtype=float32>), ('dropoff_longitude', <tf.Tensor 'ExpandDims_1:0' shape=(1000, 1) dtype=float32>), ('dropoff_latitude', <tf.Tensor 'ExpandDims:0' shape=(1000, 1) dtype=float32>), ('passenger_count', <tf.Tensor 'ExpandDims_2:0' shape=(1000, 1) dtype=float32>)])\n",
            "Consider rewriting this model with the Functional API.\n",
            "1/1 [==============================] - ETA: 0s - loss: 759.1771 - rmse: 27.5532 - mse: 759.1771WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'collections.OrderedDict'> input: OrderedDict([('pickup_longitude', <tf.Tensor 'ExpandDims_4:0' shape=(1000, 1) dtype=float32>), ('pickup_latitude', <tf.Tensor 'ExpandDims_3:0' shape=(1000, 1) dtype=float32>), ('dropoff_longitude', <tf.Tensor 'ExpandDims_1:0' shape=(1000, 1) dtype=float32>), ('dropoff_latitude', <tf.Tensor 'ExpandDims:0' shape=(1000, 1) dtype=float32>), ('passenger_count', <tf.Tensor 'ExpandDims_2:0' shape=(1000, 1) dtype=float32>)])\n",
            "Consider rewriting this model with the Functional API.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}