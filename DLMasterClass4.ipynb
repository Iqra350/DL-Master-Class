{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLMasterClass4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO9-DEO-Im5F",
        "outputId": "faa14780-ac1a-44bb-f3df-f5b4cde465aa"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os \n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2UUBOhJJkw-"
      },
      "source": [
        "npoints = 10\n",
        "x = tf.constant(range(npoints), dtype=tf.float32)\n",
        "y = 2 * x + 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGTWAhcJ-aS"
      },
      "source": [
        "# Let's define create_dataset() procedure\n",
        "# TODO 1\n",
        "def create_dataset(x,y, epochs, batch_size):\n",
        "# Using the tf.data.Dataset.from_tensor_slices() method we are able to get the slices of list or array\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U42VTkzqKrqA",
        "outputId": "da122f78-38c6-4510-fe19-fec221866ce9"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 3\n",
        "EPOCH = 2\n",
        "dataset = create_dataset(x,y,epochs=1, batch_size=3)\n",
        "\n",
        "for i, (x, y) in enumerate(dataset):\n",
        "# You can convert a native TF tensor to a NumPy array using .numpy() method\n",
        "# Let's output the value of `x` and `y`\n",
        "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
        "    assert len(x) == BATCH_SIZE\n",
        "    assert len(y) == BATCH_SIZE\n",
        "assert  EPOCH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: [0. 1. 2.] y: [10. 12. 14.]\n",
            "x: [3. 4. 5.] y: [16. 18. 20.]\n",
            "x: [6. 7. 8.] y: [22. 24. 26.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzpDD4BkVmQg"
      },
      "source": [
        "# defining the loss for the data\n",
        "def los_mse(x,y,w0,w1):\n",
        "  y_hat = w0 * x + w1\n",
        "  error = (y_hat-y)**2\n",
        "  return tf.reduce_mean(error)\n",
        "# compute the gradients for the loss\n",
        "def gradient(x,y,w0,w1):\n",
        "  with tf.GradientTape() as tape:\n",
        "    mean_loss = los_mse(x,y,w0,w1)\n",
        "    return tape.gradient(mean_loss,[w0,w1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ww4gIPNXR_V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "6a4cb62f-f6a4-4b9c-ef00-88e44437d58e"
      },
      "source": [
        "epochs = 250\n",
        "batch_size = 2\n",
        "learning_rate = 0.02\n",
        "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
        "\n",
        "\n",
        "w0 = tf.Variable(0.0)\n",
        "w1 = tf.Variable(0.0)\n",
        "dataset = create_dataset(x,y,epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "for step, (x_batch,y_batch) in enumerate(dataset):\n",
        "  dw0,dw1 = gradient(x_batch,y_batch,w0,w1)\n",
        "  w0.assign_sub(dw0*learning_rate)\n",
        "  w1.assign_sub(dw1*learning_rate)\n",
        "\n",
        "  if step%100 == 0:\n",
        "    loss = los_mse(x_batch,y_batch,w0,w1)\n",
        "    print(MSG.format(step=step,loss = loss, w0 = w0.numpy, w1 = w1.numpy()))\n",
        "\n",
        "assert loss < 0.0001\n",
        "assert abs(w0 - 2)<0.0001\n",
        "assert abs(w1 - 10) < 0.0001"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEP 0 - loss: 290.2863464355469, w0: <bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=6.0>>, w1: 0.9199999570846558\n",
            "\n",
            "STEP 100 - loss: 297.18145751953125, w0: <bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.6742325>>, w1: 1.1232171058654785\n",
            "\n",
            "STEP 200 - loss: 658.3538208007812, w0: <bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=6.5195785>>, w1: 1.6618620157241821\n",
            "\n",
            "STEP 300 - loss: 248.90643310546875, w0: <bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.654289>>, w1: 1.917733073234558\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-4fa28bac41f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}